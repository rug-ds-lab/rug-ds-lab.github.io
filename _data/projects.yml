- title: "Evaluating Server-Based and Serverless Deployment Strategies for Machine Learning Prediction Workloads in KServe"
  supervisors: ["Mahmoud Alasmar", "Alexander Lazovik"]
  available: true
  date: 2026-01-09
  type: ["bachelor-project"]
  description: |
    KServe is an open-source, Kubernetes-native framework for deploying machine learning inference services. It supports both server-based deployments using standard Kubernetes resources and serverless deployments using Knative, enabling request-driven autoscaling and scale-to-zero capabilities. This project aims to evaluate the system-level performance (latency, throughput, resource usage) of server-based and serverless deployment strategies for classical machine learning prediction workloads using KServe. The study focuses on CPU-only inference services based on scikit-learn and XGBoost models. In the first phase, representative prediction machine learning models will be trained to generate inference workloads. In the second phase, these models will be deployed using KServe under two configurations: (i) Kubernetes-based deployments with Horizontal Pod Autoscaling (HPA), and (ii) Knative-based serverless deployments with request-driven autoscaling. A stream of controlled query workloads will be generated to simulate different traffic patterns. The evaluation will focus on latency, throughput, autoscaling responsiveness, CPU and memory utilization, and cold-start overhead. The results will highlight the trade-offs between performance, scalability, and resource efficiency in server and serverless ML serving environments, and how each deployment strategy can be adapted depending on the type of workloads.
    References: <br/>
    <a href="https://arxiv.org/abs/1612.03079">Clipper: A Low-Latency Online Prediction Serving System</a>
    <a href="https://www.usenix.org/conference/atc18/presentation/oakes">SOCK: Rapid Task Provisioning with Serverless-Optimized Containers</a>
    <a href="https://www.usenix.org/conference/nsdi23/presentation/karthikeyan">SelfTune: Tuning Cluster Managers</a>
    <a href="https://kubernetes.io/docs/concepts/workloads/autoscaling/horizontal-pod-autoscale/">Horizontal Pod Autoscaling</a>
    <a href="https://knative.dev/docs/">Knative Technical Overview</a>
    <a href="https://kserve.github.io/website/docs/intro">KServe Documentation</a>
  
- title: "Estimating Inference Latency of  Deep Learning Models Using Roofline Analysis"
  supervisors: ["Mahmoud Alasmar", "Alexander Lazovik"]
  available: true
  date: 2026-01-09
  type: ["bachelor-project"]
  description: |
    Accurate estimation of inference latency is critical for meeting service-level objectives (SLOs) in large language model (LLM) serving systems. While classical ML prediction methods can be leveraged for the estimation task, their accuracy highly depends on the type of selected features. On the other hand, analytical performance models, such as the Roofline analysis, provide a hardware-aware upper bound on achievable performance; however, their applicability for latency estimation remains an open question. This project investigates how Roofline analysis can be integrated with ML prediction methods for improved estimation of end-to-end inference latency of LLM queries on a single GPU. A small set of representative LLMs will be selected, and inference latency will be measured under controlled conditions (Sequence length, batch size). Roofline-related metrics, such as arithmetic intensity and memory bandwidth utilization, will be collected using GPU profiling tools. These metrics will be used to estimate processing time and to build regression models that predict end-to-end inference latency. The evaluation will analyze prediction error, sensitivity to model size and input length, and the limitations of Roofline-based estimation.
    References: <br/>
    <a href="https://mlforsystems.org/assets/papers/neurips2024/paper28.pdf">Predicting LLM Inference Latency: A Roofline-Driven ML Method</a>

- title: "Evaluating the Performance of vLLM and DeepSpeed for Serving LLM Inference Queries"
  supervisors: ["Mahmoud Alasmar", "Alexander Lazovik"]
  available: true
  date: 2026-01-09
  type: ["master-project"]
  description: |
    The computational complexity of serving large language model (LLM) queries depends heavily on model size, sequence length, and memory access patterns. To address these challenges, several LLM inference serving frameworks have been proposed employing different optimization techniques to improve throughput and reduce memory overhead. vLLM and DeepSpeed are two prominent examples that deploy distinct techniques to achieve efficient inference serving frameworks.  vLLM proposes PagedAttention for efficient key–value cache management. On the other hand, DeepSpeed integrates multiple optimization techniques, such as parallelism and kernel-level optimizations, for scalable inference. This project aims to systematically evaluate the end-to-end inference performance (Latency, throughput, Memory footprint) of vLLM and DeepSpeed under different inference workloads.  Experiments will be performed using one of the publicly available datasets, such as ShareGPT. The results will highlight the trade-offs between KV cache management, kernel-level optimizations, and parallelism strategies in LLM inference serving, providing insights into the conditions under which each framework is most effective.
    References: <br/>
    <a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a>
    <a href="https://arxiv.org/abs/2207.00032">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</a>

- title: "Estimating Time and Resource Usage of SLURM Jobs Using RLM"
  supervisors: ["Mahmoud Alasmar", "Alexander Lazovik"]
  available: true
  date: 2026-01-09
  type: ["master-project", "master-internship"]
  description: |
    Efficient allocation of computational resources in high-performance computing (HPC) clusters requires accurate prediction of job runtime and resource requirements. Users often over-request CPU, memory, or time to avoid failures, which can lead to wasted resources and longer queue times. Therefore, predicting these requirements before job submission is critical for improving cluster utilization and scheduling efficiency. This project investigates how Regression Language Models (RLMs) can be used to estimate the time and resource usage of SLURM jobs based on submitted Bash scripts and job metadata. The study will use real job submission data from the Habrok HPC cluster.
    References: <br/>
    <a href="https://arxiv.org/abs/2509.26476">Regression Language Models for Code</a>

- title: "An Online, Continuous, Self-Adaptive Pipeline for Water Distribution Network State Estimation"
  supervisors: ["Huy Truong", "Dilek Düştegör"]
  available: true
  date: 2026-01-09
  type: ["bachelor-project", "master-project", "master-internship"]
  description: |
    Deploying machine-learning pipelines in real-world systems is challenging due to data distribution drift and inherent instability. Conventional machine-learning models typically rely on fixed weights optimized for a specific training distribution, which leads to performance degradation when exposed to unseen and noisy data in practice. To address this limitation, this group project develops a framework that supports online learning and introduces an evaluation benchmark that more closely reflects real-world operating conditions.
    Specifically, the project consists of two main components:
    1. A pipeline built around a pretrained Graph Neural Network (GNN) to estimate unknown hydraulic measurements from a limited set of sensors deployed across a water distribution network. This component focuses on implementing a Test-Time Training strategy that adapts model weights using only incoming test inputs.
    2. A benchmarking platform that simulates real-world steady-state snapshots, incorporating hydraulic measurements such as pressure, demand, and network topology across multiple water distribution systems. The benchmark is designed to evaluate the robustness and adaptability of machine-learning pipelines under what-if analyses and out-of-distribution conditions.
    
    References: <br/>
    <a href="https://proceedings.mlr.press/v119/sun20b.html">Test-Time Training with Self-Supervision for Generalization under Distribution Shifts.</a>

- title: "Multivariate State Estimation in Drinking Water Distribution Networks"
  supervisors: ["Huy Truong", "Andrés Tello", "Alexander Lazovik"]
  available: true
  date: 2026-01-09
  type: ["bachelor-project", "master-project", "master-internship"]
  description: |
    Monitoring water distribution networks plays the main role in ensuring safe drinking water delivery to millions of residents in the urban area. Traditionally, this task relies on physics-based mathematical simulations; however, such models require a large number of parameters and frequent recalibration to maintain accuracy consistent with sensor measurements. As an alternative, recent studies have proposed data-driven approaches based on Graph Neural Networks (GNNs), which leverage pressure measurements from a limited set of sensors at known locations to infer pressure values at unmonitored nodes in the network. Building on this idea, the project extends the existing univariate method to a multivariate framework, aiming to jointly estimate multiple hydraulic quantities, including pressure, demand, flow rate, head loss, and others. The candidate is expected to have a basis of machine-learning foundation and proficiency in one of the deep learning frameworks (PyTorch, TensorFlow).
    
    Reference: <br/>
    <a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2023WR036741">Graph Neural Networks for Pressure Estimation in Water Distribution Systems**.**</a>

- title: "Graph Reasoning Models"
  supervisors: ["Huy Truong", "Dilek Düştegör"]
  available: true
  date: 2026-01-09
  type: ["master-project", "master-internship"]
  description: |
    Graph Neural Networks (GNNs) have emerged as promising approaches in processing graph-based systems. GNNs leverage a message passing mechanism to update node features given neighborhood information. However, this mechanism often paired with several issues, particularly for over-smoothing, a phenomenon in which GNNs encode similar representations for all nodes in the graph. This hinders the scalability, constraining these models’ depth to a shallow level. This work explores a recursive approach to extend the number of layers virtually while measuring the impact of over-smoothing in this specific setting. The new approach is validated in the context of the water domain. Students interested in joining this project should have a basis of machine-learning knowledge and be familiar with one of deep-learning frameworks (PyTorch, Tensorflow).
    
    References:<br/>
    <a href="https://arxiv.org/pdf/2510.04871">Less is More: Recursive Reasoning with Tiny Networks.</a>
    <a href="https://iwaponline.com/jh/article/25/6/2223/98159/Assessing-the-performances-and-transferability-of">Assessing the performances and transferability of graph neural network metamodels for water distribution systems**.**</a>
    <a href="https://arxiv.org/abs/2506.21734">Hierarchical Reasoning Model**.**</a>

- title: "Evaluating LoRA for GNN-based model adaptation"
  supervisors: ["Andrés Tello", "Alexander Lazovik"]
  available: true
  date: 2026-01-05
  type: ["bachelor-project", "master-project"]
  description: |
    Foundation models have become a game-changer in several fields due to their strong generalization capabilities after some form of model adaptation, with fine-tuning being the most common approach. In this project, we aim to evaluate the effectiveness of Low-Rank Adaptation (LoRA) methods in terms of model performance, model size, and memory usage. While conventional full fine-tuning often yields high accuracy, LoRA can represent a more sustainable yet still effective alternative for model adaptation. <br /><br />
    In this project, the student will implement a LoRA-based approach to adapt a pre-trained GNN-based model to new, unseen target datasets in the context of Water Distribution Networks (WDNs). The pre-trained model has been trained on several WDNs for pressure reconstruction, and the goal is to adapt it to make predictions on unseen WDN topologies with different operating conditions. The LoRA-based adaptation will be compared against a conventional full fine-tuning approach. <br /><br />
    References:<br />
    <a href="https://openreview.net/forum?id=nZeVKeeFYf9">LoRA: Low-Rank Adaptation of Large Language Models.</a>
    <a href="https://openreview.net/pdf?id=gxhZj6uvFC">Graph low-rank adapters of high regularityfor graph neural networks and graph transformers.</a>
    <a href="https://openreview.net/pdf?id=hcoxm3Vwgy">ELoRA: Low-Rank Adaptation for Equivariant GNNs</a>

- title: "Centrality-Aware Learning vs Graph Neural Networks for State Estimation in Wastewater Systems"
  supervisors: ["Revin Naufal Alief", "Dilek Düştegör"]
  available: true
  date: 2026-01-05
  type: ["bachelor-project", "student-colloquium", "master-internship", "master-project"]
  description: |
    Wastewater systems can naturally be represented as graphs, where nodal pressure dynamics depend on both hydraulic conditions and network topology. Recent studies have demonstrated that standard deep learning models, such as multilayer perceptrons and convolutional neural networks, can achieve high pressure prediction accuracy when augmented with node centrality metrics (e.g., degree, betweenness, and closeness). These metrics implicitly encode structural information without explicitly modeling graph connectivity. However, despite promising results, such approaches have not yet been systematically compared with Graph Neural Networks (GNNs), which explicitly learn relational dependencies and are considered the natural baseline for topology-aware learning in WDNs.
    In this project, the student will investigate the extent to which centrality-aware feature engineering can serve as an alternative to GNN-based models for pressure prediction in WDNs. The project will involve a comparative study of centrality-aware neural networks and GNN architectures applied to benchmark water distribution networks under varying demand patterns and operating conditions. The analysis will focus on prediction performance, robustness to topology variations, generalization across different networks, and computational complexity.
    The project scope will be adjusted according to the study level. Bachelor-level projects will focus on reproducing existing baselines and conducting a controlled comparison between centrality-aware models and a single GNN architecture. Master-level projects may extend the analysis to include cross-network transfer learning, partial observability scenarios, and ablation studies to identify which aspects of network topology are captured by centrality metrics versus explicit graph representations. The outcomes of this project aim to clarify the practical trade-offs between implicit and explicit topology encoding in machine learning models for water infrastructure systems.
    References: (Not exhaustive)<br/>
    <a href="https://arxiv.org/abs/2106.06861">Centrality-Aware Machine Learning for Water Network Pressure Prediction.</a>
    <a href="https://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks.</a>
    <a href="https://arxiv.org/abs/1710.10903">Inductive Representation Learning on Large Graphs.</a>
    <a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks.</a>

- title: "A Systematic Review of Graph Neural Network Applications in Wastewater Systems"
  supervisors: ["Revin Naufal Alief", "Dilek Düştegör"]
  available: true
  date: 2026-01-05
  type: ["bachelor-project", "student-colloquium", "master-internship", "master-project"]
  description: |
    Graph Neural Networks (GNNs) have shown strong performance in graph-related data. If we are looking at the wastewater systems, they are able to be represented as network topology which is suitable for GNN to works on. Existing studies apply GNNs to a variety of tasks, including state estimation, forecasting, anomaly detection, and sensor placement. However, the literature remains scattered, and the overview of methods, datasets, and research gaps is still lacking. In this project, the student will conduct a systematic literature review of GNN-based approaches in wastewater systems following a structured review protocol (e.g., PRISMA). The student will identify, screen, and categorize relevant studies based on application domain, learning task, data sources (real vs synthetic), and evaluation settings. Special attention will be given to limitations related to sensor availability, uncertainty handling, and generalization across networks. The outcome of the project is a structured taxonomy and comparative analysis of existing GNN-based wastewater studies, highlighting open challenges and opportunities for future research, particularly in sensor placement and digital twin development. Project depth will be adjusted to BS or MS level.
    References:<br />
    <a href="https://ieeexplore.ieee.org/abstract/document/11231364">Graph Neural Network Empowers Intelligent Education: A Systematic Review From an Application Perspective.</a>
    <a href="https://www.bmj.com/content/372/bmj.n71">The PRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews.</a>
    <a href="https://ieeexplore.ieee.org/abstract/document/9046288">A Comprehensive Survey on Graph Neural Networks.</a>

- title: "Uncertainty Quantification in GNN-based Water Level Estimation"
  supervisors: ["Revin Naufal Alief", "Dilek Düştegör"]
  available: true
  date: 2026-01-05
  type: ["bachelor-project", "student-colloquium", "master-internship", "master-project"]
  description: |
    Graph Neural Networks (GNNs) have shown strong performance in estimating hydraulic states in partially observed wastewater networks. However, most existing approaches focus on point predictions and provide limited insight into prediction reliability, which is crucial for decision-making. In this project, the student will (1) do literature review on conformal prediction for graph-structured data, emphasizing the challenges of applying CP to graphs (e.g., exchangeability issues in node/edge settings and dependence in graph neighborhoods), and (2) implement a split conformal prediction pipeline on top of a pre-trained GNN for water level estimation in wastewater networks. Recent work such as Conformalized GNNs (CF-GNN) and conformal prediction sets for GNNs provide practical designs and evaluation protocols for graph settings. The student will evaluate uncertainty quality using empirical coverage and interval width across sensor-masking ratios and analyze how uncertainty varies across sparse sensor conditions. Project depth will be adjusted to BS or MS level.
    <br /><br /><strong>References:</strong>
    <a href="https://arxiv.org/pdf/2305.14535">Uncertainty Quantification over Graph with Conformalized Graph Neural Networks.</a>
    <a href="https://doi.org/10.1145/3711896.3737064">Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks.</a>

- title: "Sparse Sensor Placement for Graph-Based State Estimation"
  supervisors: ["Revin Naufal Alief", "Dilek Düştegör"]
  available: true
  date: 2026-01-05
  type: ["bachelor-project", "student-colloquium", "master-internship", "master-project"]
  description: |
    This project studies sparse sensor placement in large networked systems to support accurate state estimation using graph neural networks (GNNs). The focus is on identifying critical sensor locations using graph-theoretic properties and invariants, and evaluating their impact on learning-based estimation performance. Students will implement and compare sensor placement strategies and assess their effectiveness under limited sensing. Project depth will be adjusted to BS or MS level.
    <a href="https://doi.org/10.3390/w16131835">Graph Neural Networks for Sensor Placement: A Proof of Concept towards a Digital Twin of Water Distribution Systems.</a>
    <a href="https://doi.org/10.48550/arXiv.2508.00141">INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks</a>
    <a href="https://doi.org/10.1109/ACCESS.2025.3611333">Graph Neural Networks for Evaluating the Reliability and Resilience of Infrastructure Systems: A Systematic Review of Models, Applications, and Future Directions</a>

- title: "Wastewater Systems Benchmark Dataset Development"
  supervisors: ["Revin Naufal Alief", "Dilek Düştegör"]
  available: true
  date: 2026-01-05
  type: ["bachelor-project", "student-colloquium", "master-internship", "master-project"]
  description: |
    This project aims to develop an academic benchmark dataset for wastewater systems to support reproducible research. The work includes a literature review, followed by dataset creation through merging existing data or simulating diverse operating scenarios. A key outcome is a modular, automated data-generation pipeline with clear documentation, suitable for data-driven modeling and analysis tasks.
    <a href="https://doi.org/10.3390/engproc2024069050">Large-Scale Multipurpose Benchmark Datasets for Assessing Data-Driven Deep Learning Approaches for Water Distribution Networks</a>
    <a href="https://doi.org/10.1016/j.dib.2023.109148">Benchmarking dataset for leak detection and localization in water distribution systems</a>
    <a href="https://https://github.com/KIOS-Research/LeakDB">LeakDB: A Benchmark Dataset for Leakage Diagnosis in Water Distribution Networks</a>
    <a href="https://doi.org/10.1016/j.dib.2023.109148">Benchmarking Dataset for Leak Detection and Localization in Water Distribution Systems</a>

- title: "Dataset Management"
  supervisors: ["Huy Truong", "Andrés Tello"]
  available: true
  date: 2025-08-25
  type: ["bachelor-internship"]
  description: |  
    Have you ever managed a large dataset? This project provides an opportunity to handle a dataset with over 8,000 downloads each month. You will reorganize the dataset by task, document it thoroughly, and create a user-friendly interface and leaderboard. The project also involves working with HPC clusters, Hugging Face libraries, and GitHub Pages for documentation. Basic Python skills and familiarity with Linux commands are required.

- title: "DL Jobs Generator"
  supervisors: ["Kawsar Haghshenas", "Mahmoud Alasmar"]
  available: true
  date: 2024-11-01
  type: ["bachelor-internship"]
  description: |
    In this project you will implement a job generator process. As an input, a JSON configuration (JSON) file and jobs generation rate (jobs per unit time) will be provided. The configuration file contains metadata about different Deep learning jobs, such as the path to the executable file and required arguments. Your task is to design and implement a generator which works as follows: Randomly select a job from the JSON file, use the metadata of the selected job to prepare a YAML/Batch script, a script template will be provided, and submit the prepared script to another process using an RPC protocol. The rate at which a job is sampled and submitted should be equal to the given generation rate. In addition, an implementation of an RPC protocol is required, the description of the protocol will be provided. You may choose any programming language for coding, but advisably to use Python. You will be provided with a supplementary code with helper functions, RPC protocol description and script/configuration files description.

- title: "Implementation of Hardware Monitoring APIs"
  supervisors: ["Kawsar Haghshenas", "Mahmoud Alasmar"]
  available: true
  date: 2024-11-01
  type: ["bachelor-internship"]
  description: |
    In this project we are developing a resource manager framework, part of this framework is to design monitoring APIs functionalities, which gathers hardware metrics upon invocation. You will be given a code template and your task is to fill in the code for some API functionalities. Supplementary programs and helper functions will be provided to be able to test your implementation. A documentation for the required API functions including their input and output parameters will be provided. This project requires C and Python programming as well as basic operating systems knowledge.

- title: "Research packages for the formal specification and verification of process compositions"
  supervisors: ["Heerko Groefsema"]
  available: true
  date: 2024-10-21
  type: ["bachelor-internship"]
  description: |  
    For our research we implemented and use a number of Java packages that allow us to specify, unfold, and verify process compositions such as business process models and service compositions. These packages require some work, including new functionality, replacing old dependencies, adding different output formats, replacing log functionality, refactoring to use certain programming patterns, and more. In this project, we would like a number of students to improve, refactor, and add functionality. This project is available for up to 5 students, which will work on separate sub-projects such as:
      - Adding rich Event Log generation from random executions of annotated Petri net models.
      - Separating embedded data annotations and allowing execution of Petri nets using data.
      - Adding functionality for colored Petri nets.
      - Implementing improved Prime Event Structures (PES) representations of processes and unfolding (i.e., creation of PES) from Petri nets.
      - Replacing old dependencies and refactoring.

- title: "In-Network Atomic Multicast Protocol Validation and Verification."
  supervisors: ["Bochra Boughzala"]
  available: true
  date: 2024-10-29
  type: ["bachelor-internship"]
  description: |
    This project involves creating a high-performance networking application using the Intel Data Plane Development Kit (DPDK). The application will receive network packets over a high-speed interface, inspect the packet header fields, and validate the protocol properties for correctness and consistency guarantees. The tool will verify protocol correctness by making sure that all receiving nodes receive the same set of packets in the exact order, guaranteeing total order and consistent state across all nodes.<br />Prerequisites: C/C++ programming language - Networking libraries and tools for protocol analysis - Logging library for error reporting.

- title: "In-Network Data Stream Processing Serialization."
  supervisors: ["Bochra Boughzala"]
  available: true
  date: 2024-10-29
  type: ["bachelor-internship"]
  description: |
    This project involves creating a high-performance networking application using the Intel Data Plane Development Kit (DPDK). The application will read entries from a specified database, convert these entries into JSON format, and send the JSON entries as packets over a high-speed network. The focus is on achieving low-latency and high-throughput performance.<br />Prerequisites: C/C++ programming language - Database Management (e.g., PostgreSQL databases).

- title: "Benchmarking AI Workloads on GPU Cluster"
  supervisors: ["Kawsar Haghshenas", "Mahmoud Alasmar"]
  available: true
  date: 2025-01-21
  type: ["bachelor-project"]
  description: |
    Understanding the characteristics of AI workloads is essential for effective resource allocation and fault tolerance mechanisms. This project focuses on benchmarking various deep neural network (DNN) models on GPUs using different profiling and monitoring tools. You will observe and analyze their runtime behavior, identify the factors affecting model performance, and propose metrics that effectively quantify their runtime characteristics. The outcome of this project is to deliver a comprehensive study on profiling DNN models with minimal overhead and maximum accuracy.<br />
    References:<br />
    <a href="https://arxiv.org/abs/1808.08512">Gao, Wanling, et al. "Data motifs: A lens towards fully understanding big data and ai workloads." Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques. 2018., https://arxiv.org/abs/1808.08512</a>
    <a href="https://www.usenix.org/conference/osdi18/presentation/xiao">Xiao, Wencong, et al. "Gandiva: Introspective cluster scheduling for deep learning." 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018., https://www.usenix.org/conference/osdi18/presentation/xiao</a>
    <a href="https://arxiv.org/abs/2009.05257">Yang, Charlene, et al. "Hierarchical roofline performance analysis for deep learning applications." Intelligent Computing: Proceedings of the 2021 Computing Conference, Volume 2. Springer International Publishing, 2021., https://arxiv.org/abs/2009.05257</a>

- title: "Node masking in Graph Neural Networks"
  supervisors: ["Huy Truong", "Dilek Düştegör"]
  available: true
  date: 2025-01-21
  type: ["bachelor-project"]
  description: |
    Working with data in the real world often leads to missing information problems, which can negatively affect the performance of deep learning models. However, in proper ways, it can boost the expressiveness of Graph Neural Network (GNN) models in node representation learning through a technique known as Node Masking. In particular, it hides arbitrary nodal features in a graph and instructs the GNN to recover the missing parts. The student can explore diverse masking strategies, such as zero masking, random node replacement, mean-neighbor substitution, shared learnable embedding, and nodal permutation. These options above should be compared and evaluated in a graph reconstruction task that applies to a water distribution network. This study will focus on finding a generative technique that effectively enhances the performance of GNN models in semi-supervised transductive learning. Students interested in joining this project should possess a machine-learning background and a deep-learning framework.<br />
    References:<br />
    <a href="https://arxiv.org/pdf/2205.10803.pdf">Hou, Zhenyu, Xiao Liu, Yuxiao Dong, Chunjie Wang, and Jie Tang. "GraphMAE: Self-Supervised Masked Graph Autoencoders." arXiv preprint arXiv:2205.10803(2022).</a>
    <a href="https://arxiv.org/pdf/2010.01179.pdf">Abboud, Ralph, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. "The surprising power of graph neural networks with random node initialization." arXiv preprint arXiv:2010.01179 (2020).</a>
    <a href="https://arxiv.org/pdf/2104.13619.pdf">Hajgató, Gergely, Bálint Gyires-Tóth, and György Paál. "Reconstructing nodal pressures in water distribution systems with graph neural networks." arXiv preprint arXiv:2104.13619 (2021).</a>
    <a href="https://arxiv.org/pdf/2111.06377.pdf">He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. "Masked autoencoders are scalable vision learners." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000-16009. 2022.</a>

- title: Conditional planning an overview of approaches
  supervisors: ["Heerko Groefsema"]
  available: true
  date: 2025-02-05
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://www.sciencedirect.com/science/article/pii/B9780080499444500276">Peot, M. A., & Smith, D. E. (1992, January). Conditional nonlinear planning. In Artificial Intelligence Planning Systems (pp. 189-197). Morgan Kaufmann.</a>
    <a href="https://link.springer.com/chapter/10.1007/3-540-48317-9_4">Blythe, J. (1999). An overview of planning under uncertainty. Artificial intelligence today, 85-110.</a>
    <a href="https://www.jair.org/index.php/jair/article/view/10230">Rintanen, J. (1999). Constructing conditional plans by a theorem-prover. Journal of Artificial Intelligence Research, 10, 323-352.</a>
    <a href="https://dl.acm.org/doi/abs/10.5555/1642090.1642149">Karlsson, L. (2001, January). Conditional progressive planning under uncertainty. In IJCAI (pp. 431-438).</a>

- title: Verifying the data perspective of business processes
  supervisors: ["Heerko Groefsema"]
  available: true
  date: 2025-02-05
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://doi.org/10.1007/978-3-031-16171-1_2">Groefsema, H., Beest, N.R.T.P.v., Governatori, G. (2022). On the Use of the Conformance and Compliance Keywords During Verification of Business Processes. In: Business Process Management Forum. BPM 2022. Lecture Notes in Business Information Processing, vol 458. Springer.</a>
    <a href="https://doi.org/10.1016/j.compind.2019.103181">H. Groefsema, N.R.T.P. van Beest, A. Armas-Cervantes, Efficient conditional compliance checking of business process models, Computers in Industry, Volume 115, 2020.</a>
    <a href="https://doi.org/10.1145/1514894.1514924">Alin Deutsch, Richard Hull, Fabio Patrizi, and Victor Vianu. 2009. Automatic verification of data-centric business processes. In Proceedings of the 12th International Conference on Database Theory (ICDT '09).</a>
    <a href="https://doi.org/10.1109/TSE.2023.3319086">N. van Beest, H. Groefsema, A. Cryer, G. Governatori, S. C. Tosatto and H. Burke, Cross-Instance Regulatory Compliance Checking of Business Process Event Logs, in IEEE Transactions on Software Engineering, vol. 49, no. 11, pp. 4917-4931, Nov. 2023.</a>

- title: Explaining Graph Neural Networks
  supervisors: ["Andrés Tello"]
  available: true
  date: 2025-02-01
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://openreview.net/pdf?id=WIzzXCVYiH">Wang, X., & Shen, H. W. GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries. In The Twelfth International Conference on Learning Representations.</a>
    <a href="https://openreview.net/pdf?id=IjMUGuUmBI">Müller, P., Faber, L., Martinkus, K., & Wattenhofer, R. (2024). GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks. In The Twelfth International Conference on Learning Representations.</a>
    <a href="https://openreview.net/pdf?id=2Q8TZWAHv4">Lu, S., Mills, K. G., He, J., Liu, B., & Niu, D. (2024). GOAt: Explaining graph neural networks via graph output attribution. arXiv preprint arXiv:2401.14578.</a>
    <a href="https://arxiv.org/pdf/2209.07924">Wang, X., & Shen, H. W. (2022). Gnninterpreter: A probabilistic generative model-level explanation for graph neural networks. arXiv preprint arXiv:2209.07924.</a>

- title: Generalization in Graph Neural Networks
  supervisors: ["Andrés Tello"]
  available: true
  date: 2025-02-01
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://openreview.net/pdf?id=qaJxPhkYtD">Kanatsoulis, C., & Ribeiro, A. Counting Graph Substructures with Graph Neural Networks. In The Twelfth International Conference on Learning Representations.</a>
    <a href="https://openreview.net/pdf?id=4IT2pgc9v6">Liu, H., Feng, J., Kong, L., Liang, N., Tao, D., Chen, Y., & Zhang, M. (2023). One for all: Towards training one graph model for all classification tasks. arXiv preprint arXiv:2310.00149.</a>
    <a href="https://openreview.net/pdf?id=EYjfLeJL4l">Lee, H., Yoon, K., 2023. Towards better generalization with flexible representation of multi-module graph neural networks. Transactions on Machine Learning Research.</a>
    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.pdf">Yu, J., Liang, J., & He, R. (2023). Mind the Label Shift of Augmentation-based Graph OOD Generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 11620-11630).</a>

- title: Multimodality Graph Foundation Models
  supervisors: ["Huy Truong"]
  available: true
  date: 2025-01-21
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://www.researchgate.net/publication/371786545_Otter-Knowledge_benchmarks_of_multimodal_knowledge_graph_representation_learning_from_different_sources_for_drug_discovery">Lam, Hoang Thanh, et al. "Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery." *arXiv preprint arXiv:2306.12802* (2023).</a>
    <a href="https://aclanthology.org/2024.findings-emnlp.132/">Xia, Lianghao, Ben Kao, and Chao Huang. "Opengraph: Towards open graph foundation models." *arXiv preprint arXiv:2403.01121* (2024).</a>
    <a href="https://www.nature.com/articles/s42256-023-00624-6">Ektefaie, Yasha, et al. "Multimodal learning with graphs." *Nature Machine Intelligence* 5.4 (2023): 340-350.</a>

- title: Test-Time Training
  supervisors: ["Huy Truong"]
  available: true
  date: 2025-01-21
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="http://proceedings.mlr.press/v119/sun20b/sun20b.pdf">Sun, Yu, et al. "Test-time training with self-supervision for generalization under distribution shifts." International conference on machine learning. PMLR, 2020.</a>
    <a href="https://proceedings.neurips.cc/paper/2021/hash/b618c3210e934362ac261db280128c22-Abstract.html">Liu, Yuejiang, et al. "Ttt++: When does self-supervised test-time training fail or thrive?." Advances in Neural Information Processing Systems 34 (2021): 21808-21820.</a>
    <a href="https://link.springer.com/article/10.1007/s11263-024-02181-w">Liang, Jian, Ran He, and Tieniu Tan. "A comprehensive survey on test-time adaptation under distribution shifts." *International Journal of Computer Vision* (2024): 1-34.</a>
    <a href="https://arxiv.org/abs/2501.00663">Behrouz, Ali, Peilin Zhong, and Vahab Mirrokni. "Titans: Learning to Memorize at Test Time." arXiv preprint arXiv:2501.00663 (2024).</a>

- title: Cluster Scheduling for DLT workloads
  supervisors: ["Kawsar Haghshenas", "Mahmoud Alasmar"]
  available: true
  date: 2025-01-27
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://www.usenix.org/conference/osdi18/presentation/xiao">Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel et al. "Gandiva: Introspective Cluster Scheduling for Deep Learning." In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pp. 595-610. 2018.</a>
    <a href="https://www.usenix.org/conference/atc23/presentation/weng">Weng, Q., Yang, L., Yu, Y., Wang, W., Tang, X., Yang, G., & Zhang, L. (2023). Beware of Fragmentation: Scheduling {GPU-Sharing} Workloads with Fragmentation Gradient Descent. In 2023 USENIX Annual Technical Conference (USENIX ATC 23) (pp. 995-1008).</a>
    <a href="https://arxiv.org/abs/2408.08586">Zhang, X., Zhao, H., Xiao, W., Jia, X., Xu, F., Li, Y., ... & Liu, F. (2024). Rubick: Exploiting Job Reconfigurability for Deep Learning Cluster Scheduling. arXiv preprint arXiv:2408.08586.</a>
    <a href="https://www.usenix.org/conference/nsdi23/presentation/lai-fan">Lai, F., Dai, Y., Madhyastha, H. V., & Chowdhury, M. (2023). {ModelKeeper}: Accelerating {DNN} training via automated training warmup. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23) (pp. 769-785).</a>

- title: Estimating Deep Learning GPU Memory Consumption
  supervisors: ["Kawsar Haghshenas", "Mahmoud Alasmar"]
  available: true
  date: 2023-12-11
  type: ["student-colloquium"]
  description: |
    References:<br />
    <a href="https://dl.acm.org/doi/abs/10.1145/3368089.3417050">Yanjie Gao, Yu Liu, Hongyu Zhang, Zhengxian Li, Yonghao Zhu, Haoxiang Lin, and Mao Yang. "Estimating GPU memory consumption of deep learning models." In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 1342-1352. 2020.</a>
    <a href="https://ieeexplore.ieee.org/abstract/document/9755116">Haiyi Liu, Shaoying Liu, Chenglong Wen, and W. Eric Wong. "TBEM: Testing-Based GPU-Memory Consumption Estimation for Deep Learning." IEEE Access, 10, pp.39674-39680. 2022.</a>
    <a href="https://arxiv.org/abs/2205.12095">Lu Bai, Weixing Ji, Qinyuan Li, Xilai Yao, Wei Xin, and Wanyi Zhu. "Dnnabacus: Toward accurate computational cost prediction for deep neural netw." arXiv preprint arXiv:2205.12095. 2022.</a>
    <a href="https://ieeexplore.ieee.org/abstract/document/10172674">Yanjie Gao, Xianyu Gu, Hongyu Zhang, Haoxiang Lin, and Mao Yang. "Runtime performance prediction for deep learning models with graph neural network." In IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 368-380. 2023.</a>

- title: DiTEC project- Inverse problem in Water Distribution Networks
  supervisors: ["Huy Truong"]
  available: true
  date: 2025-04-03
  type: ["master-internship"]
  description: |
    Water researchers have relied on simulations to monitor the behavior of Water Distribution Networks. These simulations require a comprehensive set of parameters- such as elevation, demand, and pipe diameters-to determine hydraulic states accurately. This increases labor cost, time consumption and, therefore, poses a significant challenge. But what if we could reverse the process and let AI infer the missing pieces? Building on this idea, the project explores an innovative approach: leveraging data-driven deep learning methods to predict initial input conditions based on available output states. As a researcher on this project, the candidate will select and train a cutting-edge Graph Neural Network on a massive dataset. As a result, the model should be able to predict initial conditions while considering the structural and physical constraints. The candidate will submit the implementation code and a report detailing the problem and the proposed solution. The ideal candidate should have a background in machine learning and be familiar with at least one deep-learning framework.
    <br />References:<br />
    <a href="https://arxiv.org/abs/2503.17167">Truong, Huy, et al. "DiTEC-WDN: A Large-Scale Dataset of Hydraulic Scenarios across Multiple Water Distribution Networks." (2025).</a>

- title: Can we train a Neural Network with Forward-Forward “harmoniously”?
  supervisors: ["Huy Truong"]
  available: true
  date: 2025-04-03
  type: ["master-internship"]
  description: |
    Back Propagation(BP) is a de facto approach to training neural network models. Nevertheless, it is biologically implausible and requires complete knowledge (i.e., tracks the entire flow of information from start to end of a model) to perform a backward pass. Instead, an alternative approach called Forward-Forward(FF) can replace the backward pass with an additional forward one and update the model weights in an unsupervised fashion. In particular, FF performs forward passes using positive and negative inputs, respectively, and, therefore, employs the difference between the two activation versions at each layer in the neural network to compute the loss and update weights. Here, the project studies the behavior of FF employing different losses: (1) cross-entropy and (2) harmonic loss. Also, it is valuable to study the relevance between harmonic loss and FF in terms of distance metrics or geometric properties in an embedding space. As a deliverable, the candidate should submit a detailed report and implementation code. For primary requirements, the candidate should be familiar with one of the deep learning frameworks and have experience in setting up machine learning experiments.
    <br />References:<br />
    <a href="https://arxiv.org/abs/2212.13345">Hinton, Geoffrey. "The forward-forward algorithm: Some preliminary investigations." *arXiv preprint arXiv:2212.13345* (2022).</a>
    <a href="https://arxiv.org/html/2502.01628v1">Baek, David D., et al. "Harmonic Loss Trains Interpretable AI Models." *arXiv preprint arXiv:2502.01628* (2025).</a>

- title: Leveraging Structural Similarity for Performance Estimation of Deep Learning Training Jobs
  supervisors: ["Mahmoud Alasmar"]
  available: true
  date: 2025-04-04
  type: ["master-internship"]
  description: |
    Deep learning (DL) workload-aware schedulers make decisions based on performance data collected through a process called profiling. However, profiling each job individually is computationally expensive, reducing the practicality of such approaches. Fortunately, DL models exhibit structural similarities that can be leveraged to develop alternative methods for performance estimation. One promising approach involves representing DL models as graphs and measuring their similarity using Graph Edit Distance (GED) [1]. By analyzing the structural similarities between models, we can potentially predict the performance of one model based on the known performance of another, reducing the need for extensive profiling. In this project, you will: Study and implement the similarity matching mechanism proposed in [2], compare runtime performance of similar DL models, focusing on key metrics such as GPU utilization and power consumption, and investigate the relationship between model similarity and performance predictability, trying to answer the following question: Given two similar DL models and the performance of one, what can we infer about the performance of the other? You will work with a selected set of DL training models, and performance metrics will be collected using Nvidia's GPU profiling tools, such as DCGM.
    <br />References:<br />
    <a href="https://doi.org/10.1145/2882903.2915236">Fei Bi, Lijun Chang, Xuemin Lin, Lu Qin, and Wenjie Zhang. 2016. Efficient Subgraph Matching by Postponing Cartesian Products. In Proceedings of the 2016 International Conference on Management of Data (SIGMOD '16). Association for Computing Machinery, New York, NY, USA, 1199–1214</a>
    <a href="https://www.usenix.org/conference/nsdi23/presentation/lai-fan">Lai, F., Dai, Y., Madhyastha, H. V., & Chowdhury, M. (2023). {ModelKeeper}: Accelerating {DNN} training via automated training warmup. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23) (pp. 769-785).</a>

- title: Model Checking for Environmental Sustainability
  supervisors: ["Heerko Groefsema", "Michel Medema"]
  available: true
  date: 2025-12-05
  type: ["bachelor-project", "master-project"]
  description: |
    Organisations are increasingly concerned with environmental sustainability for various reasons (e.g., legislative, economic, ecological, or social). Quantifying sustainability performance across different dimensions is necessary for fulfilling legislative requirements and evaluating improvement efforts. In this project you will extend existing model checking approaches so that they can deal with business process models in which key environmental indicators have been attached to tasks and subprocesses along with possible target values for those indicators that should be enforced during process execution.

- title: Runtime Compliance Checking for Camunda 8
  supervisors: ["Heerko Groefsema", "Michel Medema"]
  available: true
  date: 2025-12-05
  type: ["bachelor-project", "master-internship", "master-project"]
  description: |
    Organisations are increasingly concerned with environmental sustainability for various reasons (e.g., legislative, economic, ecological, or social). Quantifying sustainability performance across different dimensions is necessary for fulfilling legislative requirements and evaluating improvement efforts. In this project you will integrate an existing compliance checking tool into the Camunda 8 platform.

- title: Verification of Security and Privacy concepts in BPMN Choreography diagrams
  supervisors: ["Heerko Groefsema"]
  available: true
  date: 2025-12-05
  type: ["bachelor-project", "master-project"]
  description: |
    Where process models define the flow of activities of participants, choreographies describe interactions between participants. Within such interactions, the security and privacy related concepts of separation of duties and division of knowledge are important. The former specifies that no one person has the privileges to misuse the system, either by error or fraudulent behavior, while the latter defines the absence of total knowledge within a single person, such that the knowledge can not be abused. The problem is, how do we specify such concepts and what kind of model is required to verify these concepts? In this project we ask the student to devise an approach to formally specify and verify these concepts given a BPMN Choreography Diagram.
    <br />References:<br />
    <a href="https://www.omg.org/spec/BPMN/2.0/PDF" rel="nofollow">OMG. Business process model and notation (BPMN) version 2.0, 2011.</a>
    <a href="https://doi.org/10.1007/978-3-319-65000-5_3" rel="nofollow">Pullonen, Pille &amp; Matulevičius, Raimundas &amp; Bogdanov, Dan. (2017). PE-BPMN: Privacy-Enhanced Business Process Model and Notation. 40-56.</a>
    <a href="https://github.com/rug-ds-lab/BPMVerification" rel="nofollow">BPMVerification package</a>

- title: Obtaining Alignments from Transition Graphs
  supervisors: ["Heerko Groefsema"]
  available: true
  date: 2025-12-05
  type: ["bachelor-project", "master-project"]
  description: |
    The practice of checking conformance of business process models has revolutionized the industry through the amount of insight it creates into the process flows of businesses. Conformance checking entails matching an event log (which details events of past executions) against a business process model (which details the prescribed process flow) through a so called alignment. Any deviation from the prescribed process flow is detected and reported. Generally, alignments are obtained by matching the so called token replay of process models (e.g., Petri nets) against events in logs. Our Transition Graphs are also obtained from token replays, but offer further insight into parallel executions than regular Reachability Graphs. As a result, we are interested in the applicability of obtaining alignments using Transition Graphs, especially when matched against event logs that include lifecycle events and thus offer parallel execution data. In this project we ask the student to implement and evaluate the applicability of such an approach.
    <br />References:<br />
    <a href="https://doi.org/10.1109/TSC.2016.2579621" rel="nofollow">H. Groefsema, N.R.T.P. van Beest, and M. Aiello (2016) A Formal Model for Compliance Verification of Service Compositions. IEEE Transactions on Service Computing.</a>
    <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-99414-7.pdf" rel="nofollow">Carmona, Josep, et al. "Conformance checking." Switzerland: Springer.[Google Scholar] (2018).</a>
    <a href="https://github.com/rug-ds-lab/BPMVerification" rel="nofollow">BPMVerification package</a>

- title: Obtaining Alignments from behavior
  supervisors: ["Heerko Groefsema"]
  available: true
  date: 2025-12-05
  type: ["master-project"]
  description: |
    The practice of checking conformance of business process models has revolutionized the industry through the amount of insight it creates into the process flows of businesses. Conformance checking entails matching an event log (which details events of past executions) against a business process model (which details the prescribed process flow) through a so called alignment. Any deviation from the prescribed process flow is detected and reported. Generally, alignments are obtained by matching the so called token replay of process models (e.g., Petri nets) against events in logs. Instead, we would like to investigate comparing event logs against a specification of ordering relations to achieve a significant performance increase. 

